{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3c779c",
   "metadata": {},
   "source": [
    "# Question-Answer Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d0238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://ragsample.blob.core.windows.net/wheels/azure_ai_tools-0.0.1-py3-none-any.whl\n",
    "!pip install azureml-rag azure-ai-ml azureml-core\n",
    "!pip install \"langchain==0.0.276\" \"nltk==3.8.1\" pandas wikipedia\n",
    "!pip install \"promptflow[azure]\" \"promptflow-tools==0.1.0.b5\" promptflow_vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1026af",
   "metadata": {},
   "source": [
    "## Setup AzureOpenAI Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92580ac4",
   "metadata": {},
   "source": [
    "Update workspace details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"subscription_id\": \"<subscription_id>\",\n",
    "    \"resource_group\": \"<resource_group_name>\",\n",
    "    \"workspace_name\": \"<workspace_name>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e54f2",
   "metadata": {},
   "source": [
    "Update Azure Open AI details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7690716",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_endpoint = \"https://<aoai-endpoint>.openai.azure.com/\" # Replace with resource endpoint URL\n",
    "aoai_key = \"<aoai-key>\"  # Replace with resource key\n",
    "completion_deployment_name = \"gpt-4\"  # Replace with deployment name of the chat completion model\n",
    "completion_model_name = \"gpt-4\"  # Replace with chat model name: gpt-4, gpt-35-turbo\n",
    "embedding_deployment_name = \"text-embedding-ada-002\"  # Replace with deployment name for text-embedding-ada-002 model\n",
    "aoai_connection_name = \"azure_open_ai_connection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41882fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Azure Open AI details in sample Prompt flow files\n",
    "chat_qna_file = \"./flows/bring_your_own_data_chat_qna/flow.dag.yaml\"\n",
    "with open(chat_qna_file) as f:\n",
    "    chat_qna_flow_dag = f.read()\n",
    "chat_qna_flow_dag = chat_qna_flow_dag.replace(\"deployment_name: 'text-embedding-ada-002'\", f\"deployment_name: '{embedding_deployment_name}'\") \\\n",
    "    .replace(\"deployment_name: 'gpt-4'\", f\"deployment_name: '{completion_deployment_name}'\") \\\n",
    "    .replace(\"connection: 'azure_open_ai_connection'\", f\"connection: '{aoai_connection_name}'\")\n",
    "with open(chat_qna_file, \"w\") as f:\n",
    "    f.write(chat_qna_flow_dag)\n",
    "\n",
    "eval_file = \"./flows/qna_gpt_similarity_eval/flow.dag.yaml\"\n",
    "with open(eval_file) as f:\n",
    "    eval_flow_dag = f.read()\n",
    "eval_flow_dag = eval_flow_dag.replace(\"deployment_name: 'gpt-4'\", f\"deployment_name: '{completion_deployment_name}'\") \\\n",
    "    .replace(\"connection: 'azure_open_ai_connection'\", f\"connection: '{aoai_connection_name}'\")\n",
    "with open(eval_file, \"w\") as f:\n",
    "    f.write(eval_flow_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ff31d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure_open_ai_connection\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.rag.utils.connections import get_connection_by_name_v2, create_connection_v2\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "try:\n",
    "    aoai_connection = get_connection_by_name_v2(ws, aoai_connection_name)\n",
    "except:\n",
    "    print(\"Creating connection...\")\n",
    "    aoai_connection = create_connection_v2(\n",
    "        workspace=ws,\n",
    "        name=aoai_connection_name,\n",
    "        category=\"AzureOpenAI\",\n",
    "        target=aoai_endpoint,\n",
    "        auth_type=\"ApiKey\",\n",
    "        credentials={\n",
    "            \"key\": aoai_key,\n",
    "        },\n",
    "        metadata={\"ApiType\": \"azure\", \"ApiVersion\": \"2023-05-15\"},\n",
    "    )\n",
    "print(aoai_connection[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201070bb",
   "metadata": {},
   "source": [
    "## Generate QA\n",
    "\n",
    "Initialize a QA data generator by passing in your Azure OpenAI details for your gpt-4 or gpt-35-turbo deployment.\n",
    "We'll use it to generate different types of QA for sample text.\n",
    "\n",
    "Supported QA types:\n",
    "\n",
    "|Type|Description|\n",
    "|--|--|\n",
    "|SHORT_ANSWER|Short answer QAs have answers that are only a few words long. These words are generally relevant details from text like dates, names, statistics, etc.|\n",
    "|LONG_ANSWER|Long answer QAs have answers that are one or more sentences long. ex. Questions where answer is a definition: What is a {topic_from_text}?|\n",
    "|BOOLEAN|Boolean QAs have answers that are either True or False.|\n",
    "|SUMMARY|Summary QAs have questions that ask to write a summary for text's title in a limited number of words. It generates just one QA.|\n",
    "|CONVERSATION|Conversation QAs have questions that might reference words or ideas from previous QAs. ex. If previous conversation was about some topicX from text, next question might reference it without using its name: How does **it** compare to topicY?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81b21c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.tools.synthetic.qa import QADataGenerator, QAType\n",
    "\n",
    "# For granular logs you may set DEBUG log level:\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "model_config = dict(\n",
    "    api_base=aoai_connection[\"properties\"][\"target\"],\n",
    "    api_key=aoai_connection[\"properties\"][\"credentials\"][\"key\"],\n",
    "    deployment=completion_deployment_name,\n",
    "    model=completion_model_name,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "qa_generator = QADataGenerator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c22f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leonardo di ser Piero da Vinci (15 April 1452 â€“ 2 May 1519) was an Italian polymath of the High Renaissance who was active as a painter, draughtsman, engineer, scientist, theorist, sculptor, and architect. While his fame initially rested on his achievements as a painter, he also became known for his notebooks, in which he made drawings and notes on a variety of subjects, including anatomy, astronomy, botany, cartography, painting, and paleontology. Leonardo is widely regarded to have been a genius who epitomized the Renaissance humanist ideal, and his collective works comprise a contribution to later generations of artists matched only by that of his younger contemporary Michelangelo.Born ou'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "wiki_title = wikipedia.search(\"Leonardo da vinci\")[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:700]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de0d5c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: When was Leonardo da Vinci born?\n",
      "A: 15 April 1452\n",
      "Q: When did Leonardo da Vinci pass away?\n",
      "A: 2 May 1519\n",
      "Q: What was Leonardo da Vinci's full name?\n",
      "A: Leonardo di ser Piero da Vinci\n",
      "Q: Which period was Leonardo da Vinci an Italian polymath of?\n",
      "A: High Renaissance\n",
      "Q: Who was Leonardo da Vinci's younger contemporary with a significant contribution to later generations of artists?\n",
      "A: Michelangelo\n"
     ]
    }
   ],
   "source": [
    "result = qa_generator.generate(\n",
    "    text=text,\n",
    "    qa_type=QAType.SHORT_ANSWER,  # Feel free to change QA type \n",
    "    num_questions=5,\n",
    ")\n",
    "for question, answer in result[\"question_answers\"]:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd03000",
   "metadata": {},
   "source": [
    "## Generate QA from files\n",
    "\n",
    "Files might have large texts that go beyond model's context lengths. They need to be split to create smaller chunks. Moreover, they should not be split mid-sentence. Such partial sentences might lead to improper QAs. We will use LangChain's `NLTKTextSplitter` to deal with these issues.\n",
    "\n",
    "We'll generate QAs to use it later in Promptflow's Bulk Test.\n",
    "\n",
    "We'll read sample markdown files from `data/data_generator_texts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b98750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\data_generator_texts\\\\text-1.md',\n",
       " 'data\\\\data_generator_texts\\\\text-2.md',\n",
       " 'data\\\\data_generator_texts\\\\text-3.md']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "texts_glob = os.path.join(\"data\", \"data_generator_texts\", \"**\", \"*\")\n",
    "files = glob.glob(texts_glob, recursive=True)\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65ee75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# download pre-trained Punkt tokenizer for sentence splitting\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d72a84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts after splitting: 7\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",  # encoding for gpt-4 and gpt-35-turbo\n",
    "    chunk_size=300,  # number of tokens to split on\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = []\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "    texts += text_splitter.split_text(data)\n",
    "print(f\"Number of texts after splitting: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494616be",
   "metadata": {},
   "source": [
    "We'll use `qa_generator.generate_async()` method to make concurrent requests to Azure Open AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3643f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "\n",
    "concurrency = 3  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "async def generate_async(text):\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=QAType.LONG_ANSWER,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )\n",
    "\n",
    "results = await asyncio.gather(*[generate_async(text) for text in texts],\n",
    "                               return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "token_usage = Counter()\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "    token_usage += result[\"token_usage\"]\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9baef423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is a compute target in Azure Machine Learning and what are its typical uses in a model development lifecycle?\n",
      "A: A compute target is a designated compute resource or environment where you run your training script or host your service deployment, which can be your local machine or a cloud-based compute resource. In a typical model development lifecycle, you start by developing and experimenting on a small amount of data using your local environment, then scale up to larger data or do distributed training using training compute targets, and finally deploy the model to a web hosting environment using deployment compute targets.\n"
     ]
    }
   ],
   "source": [
    "# preview generated QAs\n",
    "for question, answer in question_answer_list[0][:3]:\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2812814b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage: Counter({'total_tokens': 5507, 'prompt_tokens': 4794, 'completion_tokens': 713})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token usage: {token_usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e7fcb",
   "metadata": {},
   "source": [
    "## Promptflow Bulk Test and Evaluation\n",
    "\n",
    "Promptflow's Bulk Test and Evaluation lets you test & evaluate flows. It requires data in a certain format. We'll prepare this format from our generated QAs. We'll then use this data to do Bulk Test and Evaluation for Chat QnA flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50dc6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "data_dict = defaultdict(list)\n",
    "for question_answers in question_answer_list:\n",
    "    chat_history = []\n",
    "    for question, answer in question_answers:\n",
    "        # QnA columns:\n",
    "        data_dict[\"question\"].append(question)\n",
    "        data_dict[\"ground_truth\"].append(answer)  # Consider generated answer as the ground truth\n",
    "\n",
    "        # Chat QnA columns:\n",
    "        data_dict[\"chat_history\"].append(json.dumps(chat_history))\n",
    "        data_dict[\"chat_input\"].append(question)\n",
    "        chat_history.append({\"inputs\": {\"chat_input\": question}, \"outputs\": {\"chat_output\": answer}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb5ae246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "output_dir = \"./data/data_generator_output/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = output_dir + \"qa_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c8fe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_dict, columns=list(data_dict.keys()))\n",
    "data_df.to_json(output_file, lines=True, orient=\"records\")\n",
    "\n",
    "# If file already exists:\n",
    "# data_df = pd.read_json(output_file, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d2d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.azure import PFClient\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "pf = PFClient.from_config(credential=credential)  # uses config.json created at the beginning of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b965df4",
   "metadata": {},
   "source": [
    "**Note**: You'll need to create a compute instance runtime if you don't have one already. You can create it from: `Prompt flow > Runtime > Create`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3195a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = \"ci-runtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c9fd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ServiceResponseError\n",
    "\n",
    "def pf_run_with_retries(*args, **kwargs):\n",
    "    retries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            base_run = pf.run(*args, **kwargs)\n",
    "        except ServiceResponseError as e:\n",
    "            retries += 1\n",
    "            if retries < 3:\n",
    "                print(f\"Request failed: {e}. Retrying...\")\n",
    "                continue\n",
    "            raise\n",
    "        return base_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa47b5f",
   "metadata": {},
   "source": [
    "### Bulk Test\n",
    "\n",
    "We'll create Bulk Test for a **Chat QnA flow**. This flow contains a sample Vector Index which has been indexed on Compute Target documentation. For the Bulk Test we'll use our generated QAs. Each question will be fed as `chat_input` to the flow which will return the answer `outputs.chat_output`. `outputs.chat_output` can then be compared with the `ground_truth` (originally generated answer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5219a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_run = pf_run_with_retries(\n",
    "    flow=\"./flows/bring_your_own_data_chat_qna/\",\n",
    "    data=output_file,\n",
    "    column_mapping={\n",
    "        \"chat_history\": \"${data.chat_history}\",\n",
    "        \"chat_input\": \"${data.chat_input}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    ")\n",
    "\n",
    "print(\"Visualize outputs:\")\n",
    "pf.visualize(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99a85f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stream(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1611f700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.chat_input</th>\n",
       "      <th>outputs.chat_output</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the retirement date for H-series virtu...</td>\n",
       "      <td>The retirement date for H-series virtual machi...</td>\n",
       "      <td>H-series virtual machine series will be retire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can you do with Azure Machine Learning co...</td>\n",
       "      <td>With Azure Machine Learning compute, you can s...</td>\n",
       "      <td>With Azure Machine Learning compute, you can c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the purpose of a Docker container in A...</td>\n",
       "      <td>In Azure Machine Learning inference, a Docker ...</td>\n",
       "      <td>When performing inference in Azure Machine Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the capabilities of Compute cluster a...</td>\n",
       "      <td>Compute clusters are multi-node compute target...</td>\n",
       "      <td>Compute cluster supports single- or multi-node...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is a compute target in Azure Machine Lear...</td>\n",
       "      <td>A compute target in Azure Machine Learning is ...</td>\n",
       "      <td>A compute target is a designated compute resou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   inputs.chat_input  \\\n",
       "0  What is the retirement date for H-series virtu...   \n",
       "1  What can you do with Azure Machine Learning co...   \n",
       "2  What is the purpose of a Docker container in A...   \n",
       "3  What are the capabilities of Compute cluster a...   \n",
       "4  What is a compute target in Azure Machine Lear...   \n",
       "\n",
       "                                 outputs.chat_output  \\\n",
       "0  The retirement date for H-series virtual machi...   \n",
       "1  With Azure Machine Learning compute, you can s...   \n",
       "2  In Azure Machine Learning inference, a Docker ...   \n",
       "3  Compute clusters are multi-node compute target...   \n",
       "4  A compute target in Azure Machine Learning is ...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  H-series virtual machine series will be retire...  \n",
       "1  With Azure Machine Learning compute, you can c...  \n",
       "2  When performing inference in Azure Machine Lea...  \n",
       "3  Compute cluster supports single- or multi-node...  \n",
       "4  A compute target is a designated compute resou...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "try:\n",
    "    details = pf.get_details(base_run)\n",
    "    details = details.merge(data_df[[\"chat_input\", \"ground_truth\"]], left_on=[\"inputs.chat_input\"], right_on=[\"chat_input\"],\n",
    "                        how=\"inner\")\n",
    "    details = details.drop([\"inputs.chat_history\", \"chat_input\"], axis=1)\n",
    "    display(details.head(5))\n",
    "except ValueError as e:\n",
    "    # TODO: fix issue in promptflow library that causes ValueError in pf.get_details()\n",
    "    print(f\"Retrieving details failed with: {e}\")\n",
    "    print(\"Use the 'Visualize outputs' link printed above to see outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afa2a8",
   "metadata": {},
   "source": [
    "### Bulk Evaluate\n",
    "\n",
    "Now, we'll create Bulk Evaluation for the above Chat QnA flow. We'll evaluate `inputs.answer` (returned by flow) against `inputs.ground_truth` using **QnA GPT Similarity** metric. QnA GPT Similarity produces a score from 1 to 5 depending on how close (similar) `inputs.answer` is to `inputs.ground_truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a73ea878",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_run = pf_run_with_retries(\n",
    "    flow=\"./flows/qna_gpt_similarity_eval/\",\n",
    "    data=output_file,\n",
    "    run=base_run,\n",
    "    column_mapping={\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"ground_truth\": \"${data.ground_truth}\",\n",
    "        \"answer\": \"${run.outputs.chat_output}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    ")\n",
    "\n",
    "print(\"Visualize outputs:\")\n",
    "pf.visualize(eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stream(eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a79a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_details = pf.get_details(eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adf9701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in eval_details.iterrows():\n",
    "    print(\"Question:\", row[\"inputs.question\"])\n",
    "    print(\"Answer:\", row[\"inputs.answer\"])\n",
    "    print(\"Expected answer:\", row[\"inputs.ground_truth\"])\n",
    "    print(\"Similarity score:\", row[\"outputs.gpt_similarity\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3261d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp-datagen",
   "language": "python",
   "name": "tmp-datagen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
