{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: When running the first cell you will be prompted to pick your Python environment, for the pre-built container choose:\n",
    " * **Python Environment** on the first dropdown\n",
    " * **Python 3.10.x** on the second dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AIClient: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.generative import AIClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# connects to project defined in the config.json file at the root of the repo\n",
    "# use \"ai init\" to update this to point at your project\n",
    "client = AIClient.from_config(DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Azure OpenAI and Cognitive Services Connections and Set in Environment   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class WorkspaceConnection: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "# Log into the Azure CLI (run az login --use-device code) before running this step!\n",
    "default_aoai_connection = client.get_default_aoai_connection()\n",
    "default_aoai_connection.set_current_environment()\n",
    "\n",
    "# change this if you use different connection name\n",
    "default_acs_connection = client.connections.get(\"Default_CognitiveSearch\")\n",
    "default_acs_connection.set_current_environment()\n",
    "\n",
    "# change these if you use different deployment names\n",
    "# if you do that, also update the deployment name in qna_simple/langchain_model.py \n",
    "chat_model_deployment = \"gpt-35-turbo-16k-0613\"\n",
    "embedding_model_deployment = \"text-ada-embedding-002-2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MLIndex Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.generative.operations._index_data_source import LocalSource, ACSOutputConfig\n",
    "from azure.ai.generative.functions.build_mlindex import build_mlindex\n",
    "\n",
    "# build the index using the product catalog docs from data/3-product-info\n",
    "index = build_mlindex(\n",
    "    output_index_name=\"product-info-cog-search-index\",\n",
    "    vector_store=\"azure_cognitive_search\",\n",
    "    embeddings_model = f\"azure_open_ai://deployment/{embedding_model_deployment}/model/text-embedding-ada-002\",\n",
    "    data_source_url=\"https://product_info.com\",\n",
    "    index_input_config=LocalSource(input_data=\"../../data/3-product-info\"),\n",
    "    acs_config=ACSOutputConfig(\n",
    "        acs_index_name=\"product-info-index-test1\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# register the index so that it shows up in the project\n",
    "client.mlindexes.create_or_update(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement co-pilot logic using Langchain and the MLIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.rag.mlindex import MLIndex\n",
    "\n",
    "# convert MLIndex to a langchain retriever\n",
    "index_langchain_retriever = MLIndex(\n",
    "    client.mlindexes.get(name=\"product-info-cog-search-index\", label=\"latest\").path,\n",
    ").as_langchain_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a QnA function that retrieves data and uses it as context to the LLM\n",
    "def qna(question, temperature=0.0, number_of_docs=5, prompt_template=None):\n",
    "    from langchain import PromptTemplate\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        deployment_name=chat_model_deployment,\n",
    "        model_name=\"gpt-35-16k-turbo\",\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    template = \"\"\"\n",
    "    System:\n",
    "    You are an AI assistant helping users with queries related to outdoor outdooor/camping gear and clothing.\n",
    "    Use the following pieces of context to answer the questions about outdoor/camping gear and clothing as completely, correctly, and concisely as possible.\n",
    "    If the question is not related to outdoor/camping gear and clothing, just say Sorry, I only can answer question related to outdoor/camping gear and clothing. So how can I help? Don't try to make up an answer.\n",
    "    If the question is related to outdoor/camping gear and clothing but vague ask for clarifying questions.\n",
    "    Do not add documentation reference in the response.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=index_langchain_retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": prompt_template,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = qa(question)\n",
    "\n",
    "    return {\n",
    "        \"question\": response[\"query\"],\n",
    "        \"answer\": response[\"result\"],\n",
    "        \"context\": \"\\n\\n\".join([doc.page_content for doc in response[\"source_documents\"]])\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qna(\"Which tent has the highest rainfly waterproof rating?\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate LLM Response on Larger Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Loading data\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "path = os.path.join(os.getcwd() + \"/data.jsonl\")\n",
    "data = load_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artifacts': {'gpt_coherence': ['5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '1',\n",
      "                                 '5',\n",
      "                                 '3',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '3',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '1'],\n",
      "               'gpt_fluency': ['5',\n",
      "                               '5',\n",
      "                               '5',\n",
      "                               '5',\n",
      "                               '1',\n",
      "                               '4',\n",
      "                               '3',\n",
      "                               '5',\n",
      "                               '5',\n",
      "                               '3',\n",
      "                               '5',\n",
      "                               '5',\n",
      "                               '1'],\n",
      "               'gpt_groundedness': ['5',\n",
      "                                    '5',\n",
      "                                    '5',\n",
      "                                    '5',\n",
      "                                    '1',\n",
      "                                    '5',\n",
      "                                    '5',\n",
      "                                    '5',\n",
      "                                    '1',\n",
      "                                    '1',\n",
      "                                    '5',\n",
      "                                    '1',\n",
      "                                    '1'],\n",
      "               'gpt_relevance': ['5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '4',\n",
      "                                 '1',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '5',\n",
      "                                 '1'],\n",
      "               'gpt_similarity': ['5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '4',\n",
      "                                  '1',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '5',\n",
      "                                  '1']},\n",
      " 'metrics': {'exact_match': nan, 'f1_score': 0.4442860277401262}}\n",
      "Open in AI Studio: https://eastus.studio.ml.azure.com/projectEvaluation?flight=AiStudio,DeployChatWebapp,SkipAADRegistration/projectEvaluation&wsid=/subscriptions/597966d1-829f-417e-9950-8189061ec09c/resourceGroups/azureai_development/providers/Microsoft.MachineLearningServices/workspaces/dantaylo-bugbashcli\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.generative.evaluate import evaluate\n",
    "from pprint import pprint\n",
    "\n",
    "# Evaluate the default vs the improved system prompt to see if the improved prompt\n",
    "# performs consistently better across a larger set of inputs\n",
    "result = evaluate(\n",
    "    evaluation_name=\"baseline-evaluation\",\n",
    "    asset=qna,# model_uri:\n",
    "    data=data,\n",
    "    task_type=\"qa\",\n",
    "    prediction_data=\"answer\",\n",
    "    truth_data=\"truth\", # Optional\n",
    "    metrics_config={\n",
    "        \"openai_params\": {\n",
    "            \"api_version\": \"2023-05-15\",\n",
    "            \"api_base\": os.getenv(\"OPENAI_API_BASE\"),\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"deployment_id\": chat_model_deployment\n",
    "        },\n",
    "        \"questions\": \"question\",\n",
    "        \"contexts\": \"context\",\n",
    "    },\n",
    "    tracking_uri=client.tracking_uri,\n",
    ")\n",
    "pprint(result)\n",
    "\n",
    "# Print a link to open the evalautions page in AI Studio\n",
    "print(f\"Open in AI Studio: https://ml.azure.com/projectEvaluation?flight=AiStudio,DeployChatWebapp,SkipAADRegistration/projectEvaluation&wsid=/subscriptions/{client.subscription_id}/resourceGroups/{client.resource_group_name}/providers/Microsoft.MachineLearningServices/workspaces/{client.project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate copilot performance across different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is not a file but loaded data\n",
      "Error logging data as dataset, continuing without it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring failed for QA metric bertscore\n",
      "Class: MissingDependencies\n",
      "Message: bert-score packages are not available. Please run pip install azureml-metrics[bert-score]\n",
      "Scoring failed for QA metric exact_match\n",
      "Class: MissingDependencies\n",
      "Message: evaluate package is not available. Please run pip install azureml-metrics[evaluate]\n",
      "Failed to aggregate the scores for metric : bertscore with the following exception : ufunc 'add' did not contain a loop with signature matching types (dtype('<U19'), dtype('<U19')) -> None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'artifacts': {'gpt_coherence': ['5', '5', '1'],\n",
      "                'gpt_fluency': ['5', '5', '1'],\n",
      "                'gpt_groundedness': ['5', '5', '3'],\n",
      "                'gpt_relevance': ['5', '5', '1'],\n",
      "                'gpt_similarity': ['4', '4', '1']},\n",
      "  'metrics': {'exact_match': nan, 'f1_score': 0.3763303533418476}}]\n"
     ]
    }
   ],
   "source": [
    "# Sweep over different values of temperature and number_of_docs to find the best value\n",
    "# Evaluation results will be logged to project by setting tracking_uri\n",
    "result = evaluate( \n",
    "    evaluation_name=\"qna-params-eval\",\n",
    "    asset=qna,\n",
    "    data=data,\n",
    "    task_type=\"qa\",\n",
    "    prediction_data=\"answer\",\n",
    "    truth_data=\"truth\", # Optional\n",
    "    metrics_config={\n",
    "        \"openai_params\": {\n",
    "            \"api_version\": \"2023-05-15\",\n",
    "            \"api_base\": os.getenv(\"OPENAI_API_BASE\"),\n",
    "            \"api_type\": \"azure\",\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"deployment_id\": chat_model_deployment\n",
    "        },\n",
    "        \"questions\": \"question\",\n",
    "        \"contexts\": \"context\",\n",
    "    },\n",
    "    tracking_uri=client.tracking_uri,\n",
    "    params={\n",
    "        \"temperature\": [0.0, 0,1],\n",
    "        \"number_of_docs\": [5, 10]\n",
    "    }\n",
    ")\n",
    "\n",
    "pprint(result)\n",
    "\n",
    "# Print a link to open the evalautions page in AI Studio\n",
    "print(f\"Open in AI Studio: https://ml.azure.com/projectEvaluation?flight=AiStudio,DeployChatWebapp,SkipAADRegistration/projectEvaluation&wsid=/subscriptions/{client.subscription_id}/resourceGroups/{client.resource_group_name}/providers/Microsoft.MachineLearningServices/workspaces/{client.project_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Langchain QA Function to MIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download MLIndex files so they can be packaged with deployment code\n",
    "client.mlindexes.download(name=\"product-info-cog-search-index\", download_path=\"./qna_simple/mlindex\", label=\"latest\")\n",
    "\n",
    "# set the deployment name to be used, the url needs to be globally unique so include project name\n",
    "deployment_name = f\"{client.project_name}-copilot\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Check: endpoint dantaylo-bugbashcli-copilot exists\n",
      "\u001b[32mUploading mlflow_model (0.01 MBs): 100%|██████████| 10972/10972 [00:00<00:00, 20186.27it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.generative.entities.deployment import Deployment\n",
    "from azure.ai.generative.entities.models import LocalModel\n",
    "\n",
    "deployment = Deployment(\n",
    "    name=deployment_name,\n",
    "    model=LocalModel(\n",
    "        path=\"./qna_simple\",\n",
    "        conda_file=\"conda.yaml\",\n",
    "        loader_module=\"model_loader.py\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "deployment = client.deployments.create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"question\": \"Which tent has the highest rainfly waterproof rating?\", \"answer\": \"The tent with the highest rainfly waterproof rating is the tent with item_number 8.\", \"context\": \"# Information about product item_number: 1\\n\\n# Information about product item_number: 1\\n## Technical Specs\\n**Best Use**: Camping  \\n**Capacity**: 4-person  \\n**Season Rating**: 3-season  \\n**Setup**: Freestanding  \\n**Material**: Polyester  \\n**Waterproof**: Yes  \\n**Floor Area**: 80 square feet  \\n**Peak Height**: 6 feet  \\n**Number of Doors**: 2  \\n**Color**: Green  \\n**Rainfly**: Included  \\n**Rainfly Waterproof Rating**: 2000mm  \\n**Tent Poles**: Aluminum  \\n**Pole Diameter**: 9mm  \\n**Ventilation**: Mesh panels and adjustable vents  \\n**Interior Pockets**: Yes (4 pockets)  \\n**Gear Loft**: Included  \\n**Footprint**: Sold separately  \\n**Guy Lines**: Reflective  \\n**Stakes**: Aluminum  \\n**Carry Bag**: Included  \\n**Dimensions**: 10ft x 8ft x 6ft (length x width x peak height)  \\n**Packed Size**: 24 inches x 8 inches  \\n**Weight**: 12 lbs\\n\\n# Information about product item_number: 8\\n\\n# Information about product item_number: 8\\n## Technical Specs\\n**Best Use**: Camping  \\n**Capacity**: 8-person  \\n**Season Rating**: 3-season  \\n**Setup**: Freestanding  \\n**Material**: Polyester  \\n**Waterproof**: Yes  \\n**Floor Area**: 120 square feet  \\n**Peak Height**: 6.5 feet  \\n**Number of Doors**: 2  \\n**Color**: Orange  \\n**Rainfly**: Included  \\n**Rainfly Waterproof Rating**: 3000mm  \\n**Tent Poles**: Aluminum  \\n**Pole Diameter**: 12mm  \\n**Ventilation**: Mesh panels and adjustable vents  \\n**Interior Pockets**: 4 pockets  \\n**Gear Loft**: Included  \\n**Footprint**: Sold separately  \\n**Guy Lines**: Reflective  \\n**Stakes**: Aluminum  \\n**Carry Bag**: Included  \\n**Dimensions**: 12ft x 10ft x 7ft (Length x Width x Peak Height)  \\n**Packed Size**: 24 inches x 10 inches  \\n**Weight**: 17 lbs\\n\\n# Information about product item_number: 15\\n\\n# Information about product item_number: 15\\n## Features\\n- Spacious interior comfortably accommodates two people\\n- Durable and waterproof materials for reliable protection against the elements\\n- Easy and quick setup with color-coded poles and intuitive design\\n- Two large doors for convenient entry and exit\\n- Vestibules provide extra storage space for gear\\n- Mesh panels for enhanced ventilation and reduced condensation\\n- Rainfly included for added weather protection\\n- Freestanding design allows for versatile placement\\n- Multiple interior pockets for organizing small items\\n- Reflective guy lines and stake points for improved visibility at night\\n- Compact and lightweight for easy transportation and storage\\n- Double-stitched seams for increased durability\\n- Comes with a carrying bag for convenient portability\\n\\n# Information about product item_number: 1\\n\\n# Information about product item_number: 1\\n## Features\\n- Polyester material for durability\\n- Spacious interior to accommodate multiple people\\n- Easy setup with included instructions\\n- Water-resistant construction to withstand light rain\\n- Mesh panels for ventilation and insect protection\\n- Rainfly included for added weather protection\\n- Multiple doors for convenient entry and exit\\n- Interior pockets for organizing small items\\n- Reflective guy lines for improved visibility at night\\n- Freestanding design for easy setup and relocation\\n- Carry bag included for convenient storage and transportation\"}]\n"
     ]
    }
   ],
   "source": [
    "response = client.deployments.invoke(deployment_name, \"./request_file_qna_simple.json\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
